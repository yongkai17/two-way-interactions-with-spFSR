---
output: pdf_document
---

Some of the most commonly used models in supervised machine learning are linear and logistic regression models. One of the fundamental issues in these models is to select the best set of features to be included, particularly pairwise interaction terms. In this thesis, we develop a feature selection wrapper method called Two-Step SP-FSR for learning pairwise interactions. Given a supervised learning model and a specified model performance criterion, Two-Step SP-FSR first searches for an optimal set of main-effect features. Then it searches for relevant interaction features from these main effects while keeping the latter in training the learning model. Two-Step SP-FSR effectively enforces a strong hierarchy, meaning interaction features can be present only if both of its main-effect features are present. We run some computational experiments to compare Two-Step SP-FSR and other competitor methods, as well as the baseline models which are trained on full training sets, in order to detect interactions. Our competitor wrapper methods are Genetic Algorithm, Sequential Forward Selection and Sequential Floating Forward Selection. We also compare to one embedded method - `glinternet`. All competitor methods assume strong hierarchy. We evaluate their performances on test sets with root mean squared error and area under the curve (AUC) for regression and classification problems respectively. Two-Step SP-FSR outperforms all wrapper methods in each dataset. It results in a higher accuracy than the baseline learners, suggesting evidence of interactions. Though Two-Step SP-FSR does not always outperform `glinternet` in some experiments, it is more flexible since it can be extended to other learning models such as multinomial and Poisson regressions for interaction discovery.