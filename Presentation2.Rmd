---
title: 'MATH1332: Minor Thesis Presentation'
subtitle: 'Identifying Optimal Set of Pairwise Interaction Terms by SP-FSR Algorithm and Empirical Comparison with Other Methods'
author: "Yong Kai Wong"
date: "30 May 2018"
output:
  beamer_presentation:
    citation_package: natbib
    theme: "Madrid"
    colortheme: "dolphin"
    fonttheme: "structurebold"
link-citations: yes
referencecolor: blue
citationcolor: blue
bibliography: ref.bib
---

# Background

* A motivating example: do gene interactions help predict cancer type?
* How to determine interactions in high dimensions *optimally*?
* Focus on pairwise interaction terms (no quadratic terms)
* Given $p$ features with 2-way pairwise interaction terms, the number of possible features is 

$$p + {p\choose2} = \frac{p(p+1)}{2}$$

* SP-FSR algorithm [@vural] shows excellent performance in feature selection. Can we utilise it to identify interaction terms?

# Organisation of the presentation

1. Basic terminology
2. Earlier and current methods to identify pairwise interaction terms
3. SP-FSR algorithm for interaction identification
4. Experimental setup and results
5. Discussion
6. Conclusion

# Notations and terminology

* $Y$: response feature
* $X_{j}$: explanatory feature $j$ for $j=1,2,...p$
* Model formulation:

$$g(Y) = \beta_0 + \sum_{j=1}^{p}\beta_j X_{j} + \sum_{i<j}\beta_{i:j}X_{j}X_{i}$$

* A precise definition [@glinternet, p.1]: 

> \color{blue}"When a function $f(x_1, x_2)$ cannot be expressed as $h_1(x_1) + h_2(x_2)$ for some functions $h_1$ and $h_2$, we say that there is an interaction in $f$ between $x_1$ and $x_2$."\color{black}

* Introduction of "hierarchy"

# Terminology: hierarchy

@glinternet define:

\begin{center}
  \begin{tabular}{| l | p{6.5cm} |}
    \hline
     Hierarchy & Description\\
    \hline
    Strong & Interactions are only among pairs of nonzero main effects \\ \hline
    Weak   & Each interaction has only one of its main effects present \\ \hline
    Anti-hierarchical & Interactions are only among pairs of main effects that are not present \\ \hline
    Pure interaction & No main effects present; only interactions \\
    \hline
  \end{tabular}
\end{center}

# Terminology: an example of hierarchy

Consider three explanatory features: $\bold{X} = \{X_1, X_2, X_3\}$

1. Strong hierarchy: $\{X_1, X_2, X_1X_2\}$
2. Weak hierarchy:  $\{X_1, X_1X_2, X_1X_3\}$
3. Anti-hierarchical: $\{X_2, X_1X_3\}$
4. Pure interaction: $\{X_2X_3, X_1X_3, X_1X_2\}$

\color{red}In practice, how do we know? How can we detect?\color{black}

# Main methods to identify pairwise interaction terms

1. Statistical hypothesis test [@Cox1984]
2. Regularisation, e.g. LASSO
3. Wrapper Feature Selection [@wrapper]

# Statistical hypothesis test: an example

Considers two models:

1. $g(Y) = \beta_0 + \beta_1X_1 + \beta_2X_2$
2. $g(Y) = \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_{1:2}X_1X_2$

Run hypothesis on test on $\beta_{1:2} = 0$.

# Regularisation

* Let $l(y_i; \beta)$ be the **negative** log-likelihood contribution
* Elastic-net [@glmnet]

\begin{center}
  \begin{align*}
    \arg \min_{\beta}\frac{1}{n}\sum_{i=i}^{n}l(y_i; \beta) + \lambda[(1-\alpha)\parallel\beta\parallel^{2}_{2}/2+\alpha\parallel\beta\parallel_1]
  \end{align*}
\end{center}

* `glinternet` [@glinternet]: group-based LASSO ($\alpha = 1$) by imposing additional constraints on $\beta_{k:j}$ and $\beta_{j}$

# Wrapper feature selection

Extending feature selection by including interaction terms

1. SFFS: Sequential (Floating) Forward Selection [@SFFS]
2. \color{red}~~SFBS: Sequential (Floating) Backward Selection~~\color{black} [-@SFFS]
3. GA: Genetic Algorithm [@GA]
4. \color{blue} SP-FSR [-@vural]\color{black}

# SP-FSR algorithm

* Introduced by @vural
* Based on Simultaneous Perturbation Stochastic Approximation [@spall]
* Refined by @zeren
* Pseudo gradient descent method on the loss function
* `spFSR` package is now available in R [@spFSR]

# SP-FSR algorithm to identify interactions

* Assume a strong hierarchy
* Simplified version of **two-step SP-FSR** algorithm:

1. Identify the optimal set of $k$ main effects using SP-FSR
2. Search $k'$, number of interactions from $k$ main effects with SP-FSR

* $k$ and $k'$ can be determined via grid search or automatically

# Experimental setup

* Datasets with binary $Y$: Ionosphere and Sonar 
* Source: @UCI
* 80-20 training/test split
* Model: logistic regression
* Evaluation: AUC, AIC, and BIC
* Assume strong hierarchy
* Methods: GA, SFFS, SP-FSR, `glinternet`
* Run on 6 to 12 random seeds

# Experimental results: Sonar dataset

**Summary statistics**

* $n = 208, p = 60$
* Baseline test AUC: 0.6059524
* Baseline train AIC and BIC: 122.0000 and 312.1976
* SFFS tends to run into errors
* Test AUC, Train AUC, AIC, and BIC are mean values

```{r, echo = FALSE, message = FALSE, warning=FALSE}
library(tidyverse)
library(kableExtra)
df <- read.csv('final_results.csv')
df %>% 
  filter(dataset == 'Sonar') %>%
  group_by(type) %>%
  summarise(count = n(),
            meanTestAUC = mean(test_auc),
            meanTrainAUC = mean(train_auc),
            k0 = mean(p_0),
            k1 = mean(p_1),
            meanTrainAIC = mean(train_AIC),
            meanTrainBIC = mean(train_BIC)
            ) %>% arrange(meanTestAUC) %>% 
  knitr::kable(col.names = c('Method', 'Count', 'Test AUC', 'Train AUC',
                             'k', "k'", 'AIC', 'BIC')) %>%
  kable_styling(font_size = 7)
```

# 

```{r, echo = FALSE, message = FALSE, warning=FALSE}
baseline      <- read.csv('baseline_learners.csv')
p1 <- ggplot(df %>% filter(dataset == 'Sonar'), 
             aes(x = reorder(type, myorder), y = test_auc)) + geom_boxplot() + 
  labs(x = 'Method', y = 'Test AUC',
       title = 'AUC performance on Sonar dataset') + 
  geom_hline(yintercept = baseline[1, 'testAUC'],
             color = 'red', linetype = 'dashed') 

p1
```


# Experimental results: Ionosphere dataset

**Summary statistics**

* $n = 351, p = 33$
* Baseline test AUC: 0.8294574
* Baseline train AIC and BIC: 101.0075 and 224.7115
* SFFS tends to run into errors
* Test AUC, Train AUC, AIC, and BIC are mean values

```{r, echo = FALSE, message = FALSE, warning=FALSE}
df %>% 
  filter(dataset == 'ionosphere') %>%
  group_by(type) %>%
  summarise(count = n(),
            meanTestAUC = mean(test_auc),
            meanTrainAUC = mean(train_auc),
            k0 = mean(p_0),
            k1 = mean(p_1),
            meanTrainAIC = mean(train_AIC),
            meanTrainBIC = mean(train_BIC)
            ) %>% arrange(meanTestAUC) %>% 
  knitr::kable(col.names = c('Method', 'Count', 'Test AUC', 'Train AUC',
                             'k', "k'", 'AIC', 'BIC')) %>%
  kable_styling(font_size = 7)
```

#

```{r, echo = FALSE, message = FALSE, warning=FALSE}

p2 <- ggplot(df %>% filter(dataset == 'ionosphere'), 
             aes(x = reorder(type, myorder), y = test_auc)) + geom_boxplot() + 
  labs(x = 'Method', y = 'Test AUC',
       title = 'AUC performance on Ionosphere dataset') + 
  geom_hline(yintercept = baseline[2, 'testAUC'],
             color = 'red', linetype = 'dashed') 
p2

```


# Conclusion and Future works

**Conclusion**

* In identifying interactions, SP-FSR trails behinds `glinterent` in terms of accuracy measures, but it selects a smaller model on average based on information criteria.

**Future works**

* Extend experiment to continuous and multinomial $Y$
* Include assessment of underlying assumptions?
* Include quadratic and higher-order terms
* Include overidentifcation mechanism

# Acknowledgement

* Dr Vural Aksakalli (VA) as my supervisor and mentor
* VA, Dr Babak Abbasi, and Zeren D. Yenice  for `spFSR` package
* Zeren D. Yenice, Niranjan Adhikari, Vural Aksakalli, Dr Alev Taskin Gumus, and Babak Abbasi for their previous works on SP-FSR algorithm

# References
